{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dissociating task-engagement and arousal in primary auditory cortex\n",
    "**Charlie Heller and Stephen David**\n",
    "\n",
    "# Introduction\n",
    "It is well-known in the field of auditory neuroscience that the evoked spiking responses of neurons in auditory cortex are variable from trial to trial. Put another way, the same neuron will respond with different numbers of spikes to repeated presentations of identical sensory stimuli. Recent work has shown that some of this variability can be explained by task engagement. Often sound-evoked responses increase during task-engagement. This increase is thought to play a role in enhancing sensory encoding by increasing the signal to noise ratio of neurons that encode sounds relevant to the task. However, it has also been noted that other state variables, such as global arousal, can also modulate the strength of resonses in the same areas of auditory cortex auditory cortex. The interaction of these two state variables on auditory encoding is still an open question.\n",
    "\n",
    "In order to study this, our lab records neural activity from several (10-30) neurons simulataneously while both manipulating the animal's behavioral state using a tone-detection task and while monitoring the animals arousal using continuous measurements of pupil size. \n",
    "\n",
    "<img src=\"data/FIgure06_ptd_behavior.png\" alt=\"drawing\" width=\"500\"/>\n",
    "\n",
    "Today, we will use Python to learn about one analytical method to dissciate the effects of arousal from task-engagement. We will use the data from one neuron recorded in the primary auditory cortex of an awake, behaving ferret to do this.\n",
    "\n",
    "Here are PSTH responses from a neuron that appears to be modulated by both task engagement and arousal:\n",
    "\n",
    "<img src=\"data/psth_examples.png\" alt=\"drawing\" width=\"600\"/>\n",
    "\n",
    "**Problem:** Arousal state (indexed by pupil) correlates with task engagement. \n",
    "\n",
    "**Question:** Do both state variables actually produce changes in this neuron's sound-evoked activity?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "import os.path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the data\n",
    "The data has been stored in `numpy` arrays. You've worked with Numpy arrays in the past (e.g., when analyzing patch-clamp data). We have saved a vector of spike counts for one A1 neuron, along with the animal's pupil size, and the behavioral state (engaged or passive listening). Let's start by loading the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = 'data' # path relative to notebook location\n",
    "spike_counts = np.load(os.path.join(data_path, 'TAR010c-06-1.npy'))\n",
    "pupil_size = np.load(os.path.join(data_path, 'pupil.npy'))\n",
    "behavior_state = np.load(os.path.join(data_path, 'behavior.npy'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at our three arrays (`spike_counts`, `pupil_size`, and `behavior_state`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the shape of our numpy arrays\n",
    "print(\"spike_counts shape: {}\".format(spike_counts.shape))\n",
    "print(\"pupil_size shape: {}\".format(pupil_size.shape))\n",
    "print(\"behavior_state shape: {}\".format(behavior_state.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this experiment we have collected 359 trials that are 1.45 seconds long. Each trial contains one distractor sound (0.35-1.10 sec). Trials with target sounds have been removed for simplicity. \n",
    "\n",
    "We have grouped the data into 20 time bins per second (i.e., our sampling rate is 20 Hz). Twenty bins per second times 1.45 seconds gives us a total of 29 bins. This gives us the dimensions of our three arrays:\n",
    "\n",
    "* the first dimension corresponds to the trials\n",
    "* the second dimension corresponds to the time bins\n",
    "\n",
    "Let's visualize this to get a better sense of the data we're working with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.subplots allows you to easily control the layout of the subplots in your figure\n",
    "\n",
    "# create the figure and the axes objects\n",
    "f, ax = plt.subplots(1, 3, figsize=(12, 5), sharex=True, sharey=True)\n",
    "\n",
    "# plot the spike counts on the first axis\n",
    "img = ax[0].imshow(spike_counts, aspect='auto', origin='lower', extent=(0, 1.45, 1, 359))\n",
    "plt.colorbar(img, ax=ax[0])\n",
    "ax[0].set_xlabel('Time (sec)')\n",
    "ax[0].set_ylabel('Trial number')\n",
    "ax[0].set_title('Number of spikes per bin')\n",
    "\n",
    "# plot the pupil size on the first axis\n",
    "img = ax[1].imshow(pupil_size, aspect='auto', origin='lower', extent=(0, 1.45, 1, 359))\n",
    "plt.colorbar(img, ax=ax[1])\n",
    "ax[1].set_xlabel('Time (sec)')\n",
    "ax[1].set_title('Pupil diameter (mm)')\n",
    "\n",
    "# plot the behavior state on the first axis\n",
    "img = ax[2].imshow(behavior_state.astype('i'), aspect='auto', origin='lower', extent=(0, 1.45, 1, 359))\n",
    "ax[2].set_xlabel('Time (sec)')\n",
    "ax[2].set_title('Behavior\\nyellow = active, purple = passive')\n",
    "plt.colorbar(img, ax=ax[2])\n",
    "\n",
    "f.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On the y axis we've plotted stimulus repetitions, on the x axis, we've plotted time bins. For both `spike_counts` and `pupil_size`, the values of the data change continuously over the trial (albeit slowly for pupil). We can even tell from looking at `spike_counts` that there is a time window in which firing rate increases. This is between sound onset and offset. For `behavior_state`, however, we see that the values are consistent over the duration of individual stimulus repetitions (they're either 0 or 1). This is because the behavioral state of the animal (whether or not they're performing a task) never changes during a trial. So, `behavioral_state` is just a binary mask telling us when the animal was behaving or passively listening. In contrast, `spike_counts` and `pupil_size` are continuous variables. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot state-dependent PSTHs\n",
    "\n",
    "One observation based on the plots above is that the number of spikes evoked by a sound seems to correlate with both the animal's pupil size and its behavioral state. Let's take a closer look at this by plotting the PSTH (mean stimulus response base don the spike count) for the following conditions:\n",
    "\n",
    "* trials when the mean pupil is greater than its median\n",
    "* trials when the mean pupil is less than its median\n",
    "* trials when the animal is behaving (`behavioral_state == True`)\n",
    "* trials when the animal is passively listening (`behavioral_state == False`)\n",
    "\n",
    "We'll help you do this for pupil, then you can try for active/passive listening.\n",
    "\n",
    "First, we need to create a `pupil_mask` to classify each trial as large or small pupil. This will be a mask, in the same way as `behavior_state`. To do this, we're going to work with the mean pupil size on each trial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "Create a new array, called `mean_pupil_per_trial` that contains the mean value of the pupil on each trial. \n",
    "* Hint: This should have the same length as the number of trials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer\n",
    "mean_pupil_per_trial = np.mean(pupil_size, axis=1)\n",
    "mean_pupil_per_trial.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have this, let's create a `pupil_mask` (1D array of True/False values), based on if the mean value of pupil was above or below the median value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create variable to hold median pupil size\n",
    "pupil_divider = np.median(pupil_size)\n",
    "\n",
    "# create a pupil mask (like behavior_state)\n",
    "pupil_mask = mean_pupil_per_trial >= pupil_divider"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "Finally, plot an image of the mask alongside pupil to make sure this seems to have worked as we hoped, using the same image format as above. Use plotting code from above to figure out how to display both `pupil_mask` and `pupil_size` in subplots on the same graph. In this case, you'll just have two subplots: `ax[0]` and `ax[1]`.\n",
    "* Hint: your mask is one dimensional. If you want to display it as an image (like above) we need to add an axis to make it two dimensional. To do this, use `np.expand_dims`. Google this if you get stuck. Be sure to save the 2D array as a new variable, e.g., `pupil_mask_2D`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer\n",
    "\n",
    "# expand the pupil mask\n",
    "pupil_mask_2D = np.expand_dims(pupil_mask, axis=1)\n",
    "\n",
    "# visualize the mask\n",
    "f, ax = plt.subplots(1, 2, sharex=True, sharey=True)\n",
    "\n",
    "ax[0].imshow(pupil_size, aspect='auto', origin='lower', extent=(0, 1.45, 1, 359))\n",
    "ax[0].set_ylabel('Stimulus repetitions')\n",
    "ax[0].set_xlabel('Time bins')\n",
    "\n",
    "ax[1].imshow(pupil_mask_2D, aspect='auto', origin='lower', extent=(0, 1.45, 1, 359))\n",
    "ax[1].set_xlabel('Time bins')\n",
    "\n",
    "f.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should see yellow bands aligned with trials where pupil is large (if you don't, ask one of the TAs for help). Now, let's use the mask to extract only the spike data when pupil is big and save this as a new array called `big_pupil_spikes`. Do the same for small pupil and save this as an array called `small_pupil_spikes`. Recall that the length of our mask is equal to the number of trials and that our data has shape: `(nTrials, nBins)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "big_pupil_spikes = spike_counts[pupil_mask, :]\n",
    "\n",
    "# The \"~\" operator inverts boolean values\n",
    "small_pupil_spikes = spike_counts[~pupil_mask, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that in order to mask our data, we masked only the single dimension of our `spike_count` array corresponsing to trials because we know `pupil_mask` is constant for the duration of every trial. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can compute the PSTH's for each of these conditions and plot them:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "Using `np.mean()` along the appropriate axis, compute the big and small pupil PSTH's. Save them as `big_pupil_psth` and `small_pupil_psth`. \n",
    "* Hint: the final dimensions of both should be `(29,)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer\n",
    "big_pupil_psth = np.mean(big_pupil_spikes, axis=0)\n",
    "small_pupil_psth = np.mean(small_pupil_spikes, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you've got the PSTH's calculated correctly, run the code below to plot them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot them\n",
    "f, ax = plt.subplots(1,1)\n",
    "ax.plot(big_pupil_psth, color='purple')\n",
    "ax.plot(small_pupil_psth, color='orchid')\n",
    "ax.legend(['Big pupil', 'Small pupil'])\n",
    "ax.set_xlabel('Time bins')\n",
    "ax.set_ylabel('Spike counts');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cool, as we noted in our original plot of trial by trial data, the spiking response to a sound by this cell seems to depend strongly on pupil size."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "Repeat the above procedure using the `behavioral_state` mask. There will be a couple of steps required to do this:\n",
    "* First step: mask the data (remember that behavior state is constant over the duration of a trial, so you shouldn't need to use the entire `(359, 29)` mask for this, just the first column)\n",
    "    * Save the masked data as `active_spikes` and `passive_spikes`\n",
    "* Second step: compute the PSTH using `np.mean()`\n",
    "    * Save the PSTHs as `active_psth` and `passive_psth`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer\n",
    "\n",
    "# mask the data\n",
    "active_spikes = spike_counts[behavior_state[:,0], :]\n",
    "passive_spikes = spike_counts[~behavior_state[:,0], :]\n",
    "\n",
    "# compute the psth\n",
    "active_psth = np.mean(active_spikes, axis=0)\n",
    "passive_psth = np.mean(passive_spikes, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've got the behavior PSTH's calculated, let's plot the results alongside the pupil PSTH's."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(1, 2, figsize=(10, 4), sharex=True, sharey=True)\n",
    "\n",
    "ax[0].plot(big_pupil_psth, color='purple')\n",
    "ax[0].plot(small_pupil_psth, color='orchid')\n",
    "ax[0].legend(['Big pupil', 'Small pupil'])\n",
    "ax[0].set_xlabel('Time bins')\n",
    "ax[0].set_ylabel('Spike counts')\n",
    "\n",
    "ax[1].plot(active_psth, color='red')\n",
    "ax[1].plot(passive_psth, color='lightcoral')\n",
    "ax[1].legend(['Active', 'Passive'])\n",
    "ax[1].set_xlabel('Time bins')\n",
    "\n",
    "f.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just like for pupil, the spiking response also apparently depends strongly on whether or not the animal is engaged in a task. However, we saw in the beginning that both pupil and behavior state are highly correlated. How then do we dissociate which of these two variables is truly responsible for the change in firing rates we observed?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stepwise Linear Regression\n",
    "\n",
    "To disscoiate the effects of pupil and behavior state in this cell, we can perform what is called a stepwise linear regression. Essentially, this method will tell us how much variance in spike counts can be uniquely attributed to pupil alone and to behavior alone. Before we get into this, let's start by only considering pupil and performing a simple linear regression.\n",
    "\n",
    "As a refresher on linear regression, consider the cartoon example below. Ignore the code, this is just meant to be illustrative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X1 = np.random.random(100)*100-50\n",
    "X2 = np.random.random(100)*100-50\n",
    "Y = 100*(np.random.random(100)-0.5) + (3*X1) + 50\n",
    "\n",
    "f, ax = plt.subplots(1, 2, figsize=(8,4))\n",
    "ax[0].set_title('Strong linear relationship')\n",
    "ax[0].plot(X1, Y, 'k.')\n",
    "ax[0].plot(X1, 3*X1 + 50, 'r-')\n",
    "ax[0].set_xlabel('Independent variable #1 - x1')\n",
    "ax[0].set_ylabel('Dependent variable - y')\n",
    "\n",
    "ax[1].set_title('No linear relationship')\n",
    "ax[1].plot(X2, Y, 'k.')\n",
    "ax[1].axhline(50, color='red', linestyle='-')\n",
    "ax[1].set_xlabel('Independent variable #2 - x2')\n",
    "\n",
    "f.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the cartoon above, we've simulated an experiment with two *independent variables*, $x_1$ and $x_2$ (e.g., pupil and engagement), and a single *dependent variable*, $y$, whose value may depend on one or both of the independent variables. \n",
    "\n",
    "On the left, we've plotted $x_1$ against $y$ (black dots), where there is a strong positive linear relationship between them. This relationship is well modeled by a line (red),\n",
    "$$ y = m x_1 + b $$\n",
    "$m$ and $b$ are the parameters fit by the linear regression model, which for this plot are $m=3$ and $b=50$.\n",
    "\n",
    "On the right, the scatter plot looks random, and $m=0$ and $b=50$.\n",
    "\n",
    "Our first goal is to fit a linear regression model that uses a single indepdent variable, pupil size, to predict a dependent variable, spike counts. In \"regression-ese\", the model looks like this:\n",
    "$$ \\hat{r}(t) = \\beta_{1}p(t) + \\beta_{0} $$\n",
    "$\\hat{r}$ is the predicted spike count, $p(t)$ is pupil size, and $\\beta_{0}$ and $\\beta_{1}$ are model parameters. \"Beta\", $\\beta$, is the standard variable name for regression parameters. Compare this with the formula above and you'll see that $\\beta_{1}$ maps to $m$, $\\beta_{0}$ maps to $b$ and $p(t)$ maps to $x_{1}$. They are the same formula, but expressed using different notation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "To perform regression, we must first \"unwrap\" our data into a one-dimensional, continuous trace over time. In other words, we must `reshape` it from shape: `(n_repetitions, n_time_bins)` into `(n_repetitions * n_time_bins)`. Using `np.reshape`, do this for `spike_counts` and `pupil_size`. Save the results into `spike_counts_long` and `pupil_size_long`, respectively.\n",
    "* Hint: Is there a trick you can use to \"flatten\" the array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer\n",
    "spike_counts_long = np.reshape(spike_counts, -1)\n",
    "pupil_size_long = np.reshape(pupil_size, -1)\n",
    "\n",
    "# or...\n",
    "spike_counts_long = np.reshape(spike_counts, spike_counts.shape[0]*spike_counts.shape[1])\n",
    "pupil_size_long = np.reshape(pupil_size, spike_counts.shape[0]*spike_counts.shape[1])\n",
    "\n",
    "# or...\n",
    "spike_counts_long = spike_counts.flatten()\n",
    "pupil_size_long = pupil_size.flatten()\n",
    "\n",
    "# or ..\n",
    "spike_counts_long = spike_counts.ravel()\n",
    "pupil_size_long = pupil_size.ravel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have one long time vector of data, let's visualize what this looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(2, 1, figsize=(10, 4), sharex=True)\n",
    "\n",
    "ax[0].plot(spike_counts_long, color='grey')\n",
    "ax[0].set_ylabel('Spike counts')\n",
    "\n",
    "ax[1].plot(pupil_size_long, color='purple')\n",
    "ax[1].set_ylabel('pupil size')\n",
    "ax[1].set_xlabel('Time bins')\n",
    "\n",
    "f.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cool, with this view, we can clearly see that the spike count over time seems to correlate with pupil. Now, can we use pupil to predict these changes over time? To answer this question, we'll use linear regression. Linear regression is the process of finding the best fit line to the data. This is perhaps better understood when visualized as below (modeled after the above cartoons): "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(1,1)\n",
    "\n",
    "ax.plot(pupil_size_long, spike_counts_long, 'k.')\n",
    "ax.set_xlabel('Pupil size')\n",
    "ax.set_ylabel('Spike count');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Though the relationship isn't quite as strong as the cartoon above, we see that as expected, there is positive relationship between pupil size and spike count. Now, using linear regression, we can ask how much of the variance in spike counts can be explained using pupil size. To do this, we'll use the Ordinary Least Squares (`OLS`) model from the `statsmodels` package. This requires a few steps:\n",
    "* Create your input (independent) and output (dependent) variables. This is pupil size and spike count for us.\n",
    "    * Note that in order to find both a slope and intercept, you must explicitly add a constant to the independent variable!\n",
    "    * Note also, we format the input and output variables as dataframes so that the model output contains variable names that make sense\n",
    "* Create the model\n",
    "* Fit the model\n",
    "* Look at the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set our dependent variable\n",
    "Y = pd.DataFrame(data=spike_counts_long, columns=['spikeCounts'])\n",
    "\n",
    "# set our independent variable\n",
    "X = pd.DataFrame(data=pupil_size_long, columns=['pupilSize'])\n",
    "\n",
    "# fit both a slope AND intercept\n",
    "X = sm.add_constant(X)\n",
    "\n",
    "# let's pause and look at our model inputs\n",
    "print(X.head())\n",
    "print(Y.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above is pretty straightforward. However, note that all `sm.add_constant()` does, is add a column to the input of all 1's. Thus, the resulting model parameter for `const` will be just that, a constant, over all time. In our example, you can think of this as the spike count of the cell if pupil size were to be 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate the model\n",
    "model = sm.OLS(Y, X)\n",
    "\n",
    "# fit the model\n",
    "results = model.fit()\n",
    "\n",
    "# look at a summary of results\n",
    "results.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a lot of information that is output here. For our purposes, let's focus just on the variance explained by the model, `R^2`, and the model fit parameters: `const` (i.e., $\\beta_{0}$) and `pupilSize` (i.e., $\\beta_{1}$).\n",
    "\n",
    "Thus, plugging in the model fit parameters, we arrive at the following equation describing the relationship between pupil and spike counts in this cell:\n",
    "\n",
    "$$\\hat{r}(t) = 0.013 p(t) + 0.029$$\n",
    "\n",
    "where $\\hat{r}(t)$ is the predicted spike count and $p(t)$ is the pupil size.\n",
    "\n",
    "Based on the `R^2` value we see that this model does a very poor job of capturing the variability in spike counts. We would interpret this value of 0.038 as saying that pupil accounts for only 3.8 percent of the variability in spike counts. This is, perhaps, unsurprising. There is not information about the sound evoked response in this model, which is the major source of variability in the spike counts. Before we move on to a slightly more sophisticated model, let's plot the predicted response based on this model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "Search for a method of the model results called `predict` and use this to create an array of spike count predictions for our model input vector `X`. Save this as `spike_counts_pred`.\n",
    "* Why do you think we have to use `X` instead of `pupil_size_long`? Hint, what's the shape of `X`? The shape of `pupil_size_long`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer \n",
    "spike_counts_pred = results.predict(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've got the prediction, let's plot it over the true response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(13,4))\n",
    "\n",
    "ax1 = plt.subplot2grid((2, 3), (0,0), colspan=2)\n",
    "ax2 = plt.subplot2grid((2, 3), (1,0), colspan=2, sharex=ax1)\n",
    "ax3 = plt.subplot2grid((2,3), (0,2), rowspan=2)\n",
    "\n",
    "ax1.plot(spike_counts_long, color='gray')\n",
    "ax1.plot(spike_counts_pred, color='k')\n",
    "ax1.legend(['response', 'prediction'])\n",
    "ax1.set_ylabel('Spike counts')\n",
    "\n",
    "ax2.plot(pupil_size_long, color='purple')\n",
    "ax2.set_ylabel('pupil size')\n",
    "ax2.set_xlabel('Time bins')\n",
    "\n",
    "ax3.plot(pupil_size_long, spike_counts_long, 'k.')\n",
    "ax3.plot(pupil_size_long, spike_counts_pred, 'r-')\n",
    "ax3.legend(['raw data', 'prediction'])\n",
    "ax3.set_ylabel('Spike counts')\n",
    "ax3.set_xlabel('Pupil size')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, as we mentioned, the above model doesn't perform all that well. This is mainly because it doesn't take into account the stimulus evoked activity in the spike count. This is evident when you compare the gray to black trace in the above plot. The prediction (black) is smooth like pupil, while the response (gray) is much more dynamic. What we really want is a model that uses pupil to **scale** the **mean** stimulus evoked response of the neuron (i.e., the PSTH we generated earlier). In other words, we know that sound drives this cell. We would like to understand how pupil modulates the sound evoked response. Therefore, in addition to using just pupil, we'll also include a term for pupil **times** the mean response of the cell. Thus our model will look like this:\n",
    "\n",
    "$$\\hat{r}(t) = \\beta_{3} r_{0}(t) p(t) + \\beta_{2} r_{0}(t) + \\beta_{1} p(t) + \\beta_{0}$$\n",
    "\n",
    "where $\\hat{r}(t)$ is the predicted spike count, $r_{0}(t)$ is the mean spike count on each trial (PSTH), and $\\beta_{0}: \\beta_{3}$ are the parameters that will be fit by the model. \n",
    "\n",
    "This is starting to look a little complicated. We now have what is called a multiple regression model. We can also write the equation as follows:\n",
    "\n",
    "$$\\hat{r}(t) = (\\beta_{1} p(t) + \\beta_{0}) + (\\beta_{3} p(t)  + \\beta_{2}) r_{0}(t)$$\n",
    "\n",
    "Where we can now think of the entire first term as our old model (pupil only) and the second term as our new addition to incorporate the stimulus response. So really, it's like we've combined two different linear regression models into one. in the first model, our independent variable is simply $p(t)$ and in our second model the indendent variable is $r_{0}(t)p(t)$. \n",
    "\n",
    "Now, in order to implement this model, there are a couple of additional steps:\n",
    "* Compute the PSTH\n",
    "* \"tile\" the PSTH over all time so that it is the same length as `pupil_size_long`\n",
    "* Create our new `X` array\n",
    "* Fit the model as above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "Compute the psth over all stimuli of the the spike counts. Remember that our raw spike counts are saved in an array called: `spike_counts`. Save this as `psth`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer\n",
    "psth = np.mean(spike_counts, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "`tile` (hint) this psth over time to make it the same length as `pupil_size_long`. Call this new variable `psth_long`.\n",
    "* Hint - the argument to `np.tile` called `reps` must be an `int`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer\n",
    "ntile = int(pupil_size_long.shape[0]/spike_counts.shape[-1])\n",
    "psth_long = np.tile(psth, ntile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've generated `psth_long`, let's illustrate what it looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(2, 1, figsize=(10, 4), sharex=True)\n",
    "\n",
    "#ax[0].plot(spike_counts_long, color='grey')\n",
    "ax[0].plot(psth_long, color='black')\n",
    "ax[0].set_ylabel('Spike counts')\n",
    "\n",
    "ax[1].plot(pupil_size_long, color='purple')\n",
    "ax[1].set_ylabel('pupil size')\n",
    "ax[1].set_xlabel('Time bins')\n",
    "\n",
    "f.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's a bit dense. Let's zoom in to see the repetitions of the individual PSTHs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax[0].axis(xmin=0, xmax=500)\n",
    "display(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've got our $r_{0}(t)$ (`psth_long`). Let's create our new `X` DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.DataFrame({\n",
    "    'p(t)*r0(t)': pupil_size_long * psth_long,\n",
    "    'r0(t)': psth_long,\n",
    "    'p(t)': pupil_size_long,\n",
    "})\n",
    "X = sm.add_constant(X)\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now our independent variable has 4 columns, one for each of the model parameters $\\beta_{0}:\\beta{3}$ that we are trying to find."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "Create the model, fit the model, and look at a summary of our results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer\n",
    "model = sm.OLS(Y, X)\n",
    "\n",
    "results = model.fit()\n",
    "\n",
    "results.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model accounts for 28.4 percent of the variance in spike counts. Thus, it performs significantly better than the first model. \n",
    "\n",
    "Let's look at the prediction as we did before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create prediction\n",
    "pred = results.predict(X).values\n",
    "\n",
    "f, ax = plt.subplots(2, 1, figsize=(10,4), sharex=True)\n",
    "\n",
    "ax[0].plot(spike_counts_long, color='gray')\n",
    "ax[0].plot(pred, color='k')\n",
    "ax[0].legend(['response', 'prediction'])\n",
    "ax[0].set_ylabel('Spike counts')\n",
    "\n",
    "ax[1].plot(pupil_size_long, color='purple')\n",
    "ax[1].set_ylabel('pupil size')\n",
    "ax[1].set_xlabel('Time bins')\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nice. It's clear from this view that this second model does indeed capture the activity of this cell better than the first model. One last way we can visualize this is by plotting the pupil-dependent PSTH of the model fit on top of the true pupil-dependent PSTH to see if the model captures the changes we observed before. To to this, let's first compute the pupil-dependent PSTH of the prediction. This will require `reshape`ing our prediction, masking it, then computing the mean. For the sake of time, we've done this for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reshape the data\n",
    "pred_folded = pred.reshape(spike_counts.shape[0], spike_counts.shape[1])\n",
    "\n",
    "# mask the data\n",
    "big_pupil_pred_spikes = pred_folded[pupil_mask, :]\n",
    "small_pupil_pred_spikes = pred_folded[~pupil_mask, :]\n",
    "\n",
    "# compute the psth\n",
    "big_pupil_pred_psth = np.mean(big_pupil_pred_spikes, axis=0)\n",
    "small_pupil_pred_psth = np.mean(small_pupil_pred_spikes, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can plot this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot them\n",
    "f, ax = plt.subplots(1,1)\n",
    "ax.plot(big_pupil_psth, color='purple')\n",
    "ax.plot(small_pupil_psth, color='orchid')\n",
    "ax.plot(big_pupil_pred_psth, '--', color='purple')\n",
    "ax.plot(small_pupil_pred_psth, '--', color='orchid')\n",
    "ax.legend(['Big pupil', 'Small pupil', 'Big pupil pred', 'Small pupil pred'])\n",
    "ax.set_xlabel('Time bins')\n",
    "ax.set_ylabel('Spike counts')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cool! Our predictions overlap nicely with the true pupil-dependent PSTH's. However, as we noted earlier, our goal is to determine if this effect is due to pupil or to task-engagement. To dissociate these effects, we will create yet another set of linear regression models, this time with `behavior_state` **and** `pupil_size` as predictors. \n",
    "\n",
    "We will then remove each predictor one by one (in a stepwise manner) and analyse how `R^2` changes as a result. The way this is done is by **shuffling** (randomly reordering) each predictor in time. If there is any relationship between the predictor and the spike count, model performance should decrease for the shuffled model relative to the non-shuffled model. Shuffling keeps the degrees of freedom in the model the same and thus allows for simpler comparison of performance with the original (unshuffled) model. \n",
    "\n",
    "In short, this means we will need to create four models to answer our question:\n",
    "* Pupil not shuffled and Behavior not shuffled model (full model)\n",
    "* Pupil shuffled and Behavior not shuffled model (behavior only model)\n",
    "* Pupil not shuffled and Behavior shuffled model (pupil only model)\n",
    "* Pupil shuffled and Behavior shuffled model (shuffled model)\n",
    "\n",
    "Where the full model looks like this:\n",
    "\n",
    "$$\\hat{r}(t) = [ \\beta_{3} b(t) + \\beta_{1} p(t) + \\beta_{0} ] + \n",
    " [ \\beta_{6} b(t)  + \\beta_{5} p(t)  + \\beta_{4} ] r_{0}(t) $$\n",
    "\n",
    "Again, this looks complicated, but all we've done is replicate the first two terms from the model above and replaced $p(t)$ with $b(t)$ for behavioral state. That makes six free regression parameters, but the model fitting procedure is the same as before.\n",
    "\n",
    "We'll start this off for you by creating the full model. But first, we need to unwrap `behavior_state` into a long vector, like `pupil_size_long`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "behavior_state_long = behavior_state.ravel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now our situation looks something like this where we have two state variables (pupil and behavior) that we can use to predict the spike count in a given time bin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(3, 1, sharex=True)\n",
    "\n",
    "ax[0].plot(spike_counts_long, color='grey')\n",
    "ax[0].set_ylabel('Spike counts')\n",
    "\n",
    "ax[1].plot(pupil_size_long, color='purple')\n",
    "ax[1].set_ylabel('pupil size')\n",
    "\n",
    "ax[2].plot(behavior_state_long, color='red')\n",
    "ax[2].set_ylabel('behavior state')\n",
    "ax[2].set_xlabel('Time bins')\n",
    "\n",
    "f.set_figwidth(10)\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first create the variables we'll need to build our models (i.e. we need to combine pupil with the psth and behavior with the psth to capture the evoked auditory responses of the cell):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dependent variables\n",
    "behavior_signal = psth_long * behavior_state_long\n",
    "pupil_signal = psth_long * pupil_size_long"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can build our models.\n",
    "\n",
    "**Full-model:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_full = pd.DataFrame({\n",
    "    'b(t)*r0(t)': behavior_state_long * psth_long,\n",
    "    'p(t)*r0(t)': pupil_size_long * psth_long,\n",
    "    'b(t)': behavior_state_long.astype('f'),\n",
    "    'r0(t)': psth_long,\n",
    "    'p(t)': pupil_size_long,\n",
    "})\n",
    "X_full = sm.add_constant(X_full)\n",
    "X_full.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = spike_counts_long\n",
    "\n",
    "full_model = sm.OLS(Y, X_full)\n",
    "results_full = full_model.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Behavior model (pupil shuffled):**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffle the pupil\n",
    "pupil_shuffled = pupil_size_long.copy()\n",
    "np.random.shuffle(pupil_shuffled) # shuffles in place\n",
    "\n",
    "# create the model as before\n",
    "X_behavior = pd.DataFrame({\n",
    "    'b(t)*r0(t)': behavior_state_long * psth_long,\n",
    "    'p(t)*r0(t)': pupil_shuffled * psth_long,\n",
    "    'b(t)': behavior_state_long.astype('f'),\n",
    "    'r0(t)': psth_long,\n",
    "    'p(t)': pupil_shuffled,\n",
    "})\n",
    "X_behavior = sm.add_constant(X_behavior)\n",
    "\n",
    "behavior_model = sm.OLS(Y, X_behavior)\n",
    "\n",
    "results_behavior = behavior_model.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "Following the previous two examples, create the pupil model by shuffling behavior and create the shuffled model by shuffling both pupil and behavior.\n",
    "\n",
    "**Pupil model (behavior shuffled):**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer\n",
    "# shuffle behavior\n",
    "behavior_shuffled = behavior_state_long.copy()\n",
    "np.random.shuffle(behavior_shuffled) # shuffles in place\n",
    "behavior_signal_shuffled = psth_long * behavior_shuffled\n",
    "\n",
    "# create the model as before\n",
    "X_pupil = pd.DataFrame({\n",
    "    'b(t)*r0(t)': behavior_shuffled * psth_long,\n",
    "    'p(t)*r0(t)': pupil_size_long * psth_long,\n",
    "    'b(t)': behavior_shuffled.astype('f'),\n",
    "    'r0(t)': psth_long,\n",
    "    'p(t)': pupil_size_long,\n",
    "})\n",
    "X_pupil = sm.add_constant(X_pupil)\n",
    "\n",
    "pupil_model = sm.OLS(Y, X_pupil)\n",
    "\n",
    "results_pupil = pupil_model.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Shuffled model (both shuffled)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer\n",
    "X_shuffled = pd.DataFrame({\n",
    "    'b(t)*r0(t)': behavior_shuffled * psth_long,\n",
    "    'p(t)*r0(t)': pupil_shuffled * psth_long,\n",
    "    'b(t)': behavior_shuffled.astype('f'),\n",
    "    'r0(t)': psth_long,\n",
    "    'p(t)': pupil_shuffled,\n",
    "})\n",
    "X_shuffled = sm.add_constant(X_shuffled)\n",
    "\n",
    "shuffled_model = sm.OLS(Y, X_shuffled)\n",
    "\n",
    "results_shuffled = shuffled_model.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's take a look at the `R^2` values for the different models. (Note that we can access these as attributes of `results_` like so: `results_behavior.rsquared`. This is a nice alternative to printing our the entire summary table as we did above (though that contains useful information too)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(1,1)\n",
    "ax.plot([0, 1, 2, 3], [results_full.rsquared, results_behavior.rsquared, results_pupil.rsquared,\n",
    "                      results_shuffled.rsquared], 'o-', color='k')\n",
    "ax.axhline(results_shuffled.rsquared, color='k', linestyle='--', alpha=0.5)\n",
    "ax.axhline(results_full.rsquared, color='k', linestyle='--', alpha=0.5)\n",
    "ax.set_xticks([0, 1, 2, 3])\n",
    "ax.set_xticklabels(['full', 'behavior', 'pupil', 'shuffled'])\n",
    "ax.set_ylabel('R^2')\n",
    "ax.set_xlabel('Model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this, we see that the pupil alone accounts for the same amount of variance as the full model. The behavior model alone does better than the psth alone (shuffled model), but not as well as pupil or the shuffled model. Because there is no improvement from pupil alone to full model, we would classify this cell as an entirely pupil dependent cell. The only reason behavior does better than the psth alone is because behavior is correlated with pupil. Let's visualize this in another way using our PSTH plots. To do this, let's first create the state-dependent PSTH predictions from the pupil only model and behavior only model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get predictions\n",
    "behavior_pred = results_behavior.predict(X_behavior).values\n",
    "pupil_pred = results_pupil.predict(X_pupil).values\n",
    "\n",
    "# reshape predictions\n",
    "behavior_pred = behavior_pred.reshape(spike_counts.shape[0], spike_counts.shape[1])\n",
    "pupil_pred = pupil_pred.reshape(spike_counts.shape[0], spike_counts.shape[1])\n",
    "\n",
    "# compute psth of predictions\n",
    "\n",
    "# for behavior only model\n",
    "active_behavior_pred_psth = np.mean(behavior_pred[behavior_state[:, 0], :], axis=0)\n",
    "passive_behavior_pred_psth = np.mean(behavior_pred[~behavior_state[:, 0], :], axis=0)\n",
    "\n",
    "big_pupil_behavior_pred_psth = np.mean(behavior_pred[pupil_mask, :], axis=0)\n",
    "small_pupil_behavior_pred_psth = np.mean(behavior_pred[~pupil_mask, :], axis=0)\n",
    "\n",
    "# for pupil only model\n",
    "active_pupil_pred_psth = np.mean(pupil_pred[behavior_state[:, 0], :], axis=0)\n",
    "passive_pupil_pred_psth = np.mean(pupil_pred[~behavior_state[:, 0], :], axis=0)\n",
    "\n",
    "big_pupil_pupil_pred_psth = np.mean(pupil_pred[pupil_mask, :], axis=0)\n",
    "small_pupil_pupil_pred_psth = np.mean(pupil_pred[~pupil_mask, :], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(2,2)\n",
    "\n",
    "ax[0,0].plot(big_pupil_psth, color='purple')\n",
    "ax[0,0].plot(small_pupil_psth, color='orchid')\n",
    "ax[0,0].plot(big_pupil_pupil_pred_psth, '--', color='purple')\n",
    "ax[0,0].plot(small_pupil_pupil_pred_psth, '--', color='orchid')\n",
    "ax[0,0].legend(['Big pupil', 'Small pupil', 'Big pred', 'Small pred'])\n",
    "ax[0,0].set_ylabel('Spike counts')\n",
    "ax[0,0].set_title('Pupil model')\n",
    "\n",
    "ax[0,1].plot(active_psth, color='red')\n",
    "ax[0,1].plot(passive_psth, color='lightcoral')\n",
    "ax[0,1].plot(active_pupil_pred_psth, '--', color='red')\n",
    "ax[0,1].plot(passive_pupil_pred_psth, '--', color='lightcoral')\n",
    "ax[0,1].legend(['Active', 'Passive', 'Active pred', 'Passive pred'])\n",
    "ax[0,1].set_title('Pupil model')\n",
    "\n",
    "ax[1,0].plot(big_pupil_psth, color='purple')\n",
    "ax[1,0].plot(small_pupil_psth, color='orchid')\n",
    "ax[1,0].plot(big_pupil_behavior_pred_psth, '--', color='purple')\n",
    "ax[1,0].plot(small_pupil_behavior_pred_psth, '--', color='orchid')\n",
    "ax[1,0].legend(['Big pupil', 'Small pupil', 'Big pred', 'Small pred'])\n",
    "ax[1,0].set_xlabel('Time bins')\n",
    "ax[1,0].set_ylabel('Spike counts')\n",
    "ax[1,0].set_title('Behavior model')\n",
    "\n",
    "ax[1,1].plot(active_psth, color='red')\n",
    "ax[1,1].plot(passive_psth, color='lightcoral')\n",
    "ax[1,1].plot(active_behavior_pred_psth, '--', color='red')\n",
    "ax[1,1].plot(passive_behavior_pred_psth, '--', color='lightcoral')\n",
    "ax[1,1].legend(['Active', 'Passive', 'Active pred', 'Passive pred'])\n",
    "ax[1,1].set_xlabel('Time bins')\n",
    "ax[1,1].set_title('Behavior model')\n",
    "\n",
    "\n",
    "f.set_figwidth(10)\n",
    "f.set_figheight(7)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected from our `R^2` output, the Behavior model alone can't capture the pupil dependent changes in the PSTH (lower left panel). However, the pupil model alone **can** capture the behavior changes in the PSTH (upper right). This is just another way of illustrating that the spike count changes in this cell are truly just due to pupil, not task-engagement."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
